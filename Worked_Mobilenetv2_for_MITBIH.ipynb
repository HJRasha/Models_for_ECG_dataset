{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEodn-DZk-Pn",
        "outputId": "51e50739-2536-477c-8e10-3ac1dbb2aa3a"
      },
      "id": "sEodn-DZk-Pn",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "24fdb2b3",
      "metadata": {
        "id": "24fdb2b3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, InputLayer, Dropout, Conv1D, Conv2D, Flatten, Reshape, MaxPooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D, MaxPooling2D, AveragePooling2D, BatchNormalization, TimeDistributed, Permute, ReLU, Softmax\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "21fd391f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21fd391f",
        "outputId": "ab563151-84db-4ab2-d724-9763332682af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing index 0 for ECG : 'normal'.\n",
            "130560\n",
            "\tThere are 26 recordings of the normal ECG.\n",
            "Processing index 1 for ECG : 'AF'.\n",
            "345000\n",
            "\tThere are 69 recordings of the AF ECG.\n",
            "(95, 5000)\n",
            "(95, 2)\n",
            "Data set parsing and preparation complete.\n"
          ]
        }
      ],
      "source": [
        "SEED = 1337\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# the list of gestures that data is available for\n",
        "CLASS = [\n",
        "    \"normal\",\n",
        "    \"AF\"\n",
        "]\n",
        "\n",
        "SAMPLES_PER_PERSON = 5000\n",
        "\n",
        "NUM_CLASSES = len(CLASS)\n",
        "\n",
        "# create a one-hot encoded matrix that is used in the output\n",
        "ONE_HOT_ENCODED_CLASSES = np.eye(NUM_CLASSES)\n",
        "\n",
        "inputs = []\n",
        "outputs = []\n",
        "\n",
        "# read each csv file and push an input and output\n",
        "for ecg_index in range(NUM_CLASSES):\n",
        "  ecg = CLASS[ecg_index]\n",
        "  print(f\"Processing index {ecg_index} for ECG : '{ecg}'.\")\n",
        "\n",
        "  output = ONE_HOT_ENCODED_CLASSES[ecg_index]\n",
        "\n",
        "  # Construct the file path for the current ECG class\n",
        "  file_path = f\"/content/drive/MyDrive/Final_{ecg}_data.csv\"  #D:\\Varsity\\Thesis\\ECG dataset\\Physionet\n",
        "\n",
        "  df = pd.read_csv(file_path)\n",
        "\n",
        "  # calculate the number of gesture recordings in the file\n",
        "  num_recordings = int(df.shape[0] / SAMPLES_PER_PERSON)\n",
        "  print(df.shape[0])\n",
        "  print(f\"\\tThere are {num_recordings} recordings of the {ecg} ECG.\")\n",
        "\n",
        "  for i in range(num_recordings):\n",
        "    tensor = []\n",
        "    for j in range(SAMPLES_PER_PERSON):\n",
        "      index = i * SAMPLES_PER_PERSON + j\n",
        "      # normalize the input data, between 0 to 1:\n",
        "\n",
        "      tensor += [\n",
        "          df['ECG1'][index],\n",
        "      ]\n",
        "\n",
        "    inputs.append(tensor)\n",
        "    outputs.append(output)\n",
        "\n",
        "# convert the list to numpy array\n",
        "inputs = np.array(inputs)\n",
        "outputs = np.array(outputs)\n",
        "print(inputs.shape)\n",
        "print(outputs.shape)\n",
        "#print(inputs)\n",
        "#print(outputs)\n",
        "\n",
        "print(\"Data set parsing and preparation complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d9d7ae43",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9d7ae43",
        "outputId": "c3684f44-a5d9-4561-b82d-b153900162e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "95\n",
            "(57, 5000)\n",
            "(19, 5000)\n",
            "(19, 5000)\n",
            "Data set randomization and splitting complete.\n"
          ]
        }
      ],
      "source": [
        "# Randomize the order of the inputs, so they can be evenly distributed for training, testing, and validation\n",
        "# https://stackoverflow.com/a/37710486/2020087\n",
        "\n",
        "\n",
        "num_inputs = len(inputs)\n",
        "print(num_inputs)\n",
        "randomize = np.arange(num_inputs)\n",
        "np.random.shuffle(randomize)\n",
        "\n",
        "# Swap the consecutive indexes (0, 1, 2, etc) with the randomized indexes\n",
        "inputs = inputs[randomize]\n",
        "outputs = outputs[randomize]\n",
        "\n",
        "\n",
        "# Split the recordings (group of samples) into three sets: training, testing and validation\n",
        "TRAIN_SPLIT = int(0.6 * num_inputs)\n",
        "TEST_SPLIT = int(0.2 * num_inputs + TRAIN_SPLIT)\n",
        "\n",
        "inputs_train, inputs_test, inputs_validate = np.split(inputs, [TRAIN_SPLIT, TEST_SPLIT])\n",
        "Y_train, Y_test, Y_validate = np.split(outputs, [TRAIN_SPLIT, TEST_SPLIT])\n",
        "\n",
        "print(inputs_train.shape)\n",
        "print(inputs_validate.shape)\n",
        "print(inputs_test.shape)\n",
        "\n",
        "\n",
        "print(\"Data set randomization and splitting complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f3c0dbce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3c0dbce",
        "outputId": "d58669bd-57c3-413e-c086-b8b1e0ded176"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(57, 5000)\n",
            "(19, 5000)\n",
            "(19, 5000)\n",
            "(57, 20, 250)\n",
            "(19, 20, 250)\n",
            "(19, 20, 250)\n"
          ]
        }
      ],
      "source": [
        "window_size = 250\n",
        "segment_no = 20\n",
        "\n",
        "print(inputs_train.shape)\n",
        "print(inputs_test.shape)\n",
        "print(inputs_validate.shape)\n",
        "\n",
        "X_train = np.reshape(inputs_train, (-1, segment_no, window_size))\n",
        "X_test = np.reshape(inputs_test, (-1, segment_no, window_size))\n",
        "X_validate = np.reshape(inputs_validate, (-1, segment_no, window_size))\n",
        "\n",
        "#print(inputs_train.shape)\n",
        "#print(inputs_train[0][0][0])\n",
        "#print(len(inputs_train[0][0]))\n",
        "print(X_train.shape)\n",
        "print(X_validate.shape)\n",
        "print(X_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "#print(inputs_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "33f3e0c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33f3e0c8",
        "outputId": "8afbd332-e3c7-4ef5-9ce1-0de31afab64b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1140, 250)\n",
            "(1140, 2)\n",
            "(380, 250)\n",
            "(380, 2)\n",
            "(380, 250)\n",
            "(380, 2)\n",
            "(1140, 250, 1)\n",
            "(1140, 2)\n",
            "(380, 250, 1)\n",
            "(380, 2)\n",
            "(380, 250, 1)\n",
            "(380, 2)\n"
          ]
        }
      ],
      "source": [
        "# Flatten the 2D features to 1D\n",
        "num_features = X_train.shape[-1]\n",
        "num_samples = X_train.shape[0]\n",
        "inputs_train = X_train.reshape(-1, num_features)\n",
        "outputs_train = Y_train.repeat(20, axis=0)  # Repeat the labels for each array\n",
        "\n",
        "\n",
        "# Similarly preprocess validation and test data\n",
        "inputs_validate = X_validate.reshape(-1, num_features)\n",
        "outputs_validate = Y_validate.repeat(20, axis=0)\n",
        "inputs_test = X_test.reshape(-1, num_features)\n",
        "outputs_test = Y_test.repeat(20, axis=0)\n",
        "\n",
        "print(inputs_train.shape)\n",
        "print(outputs_train.shape)\n",
        "print(inputs_validate.shape)\n",
        "print(outputs_validate.shape)\n",
        "print(inputs_test.shape)\n",
        "print(outputs_test.shape)\n",
        "\n",
        "\n",
        "inputs_train = inputs_train.reshape((1140, 250, 1))\n",
        "inputs_validate = inputs_validate.reshape((380, 250, 1))\n",
        "inputs_test = inputs_test.reshape((380, 250, 1))\n",
        "\n",
        "\n",
        "\n",
        "print(inputs_train.shape)\n",
        "print(outputs_train.shape)\n",
        "print(inputs_validate.shape)\n",
        "print(outputs_validate.shape)\n",
        "print(inputs_test.shape)\n",
        "print(outputs_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ba24bab6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba24bab6",
        "outputId": "7d33c329-9095-4568-97a5-f0819bdf5ba6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 250, 1)]          0         \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 125, 32)           128       \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 125, 192)          6336      \n",
            "                                                                 \n",
            " depthwise_conv1d (Depthwis  (None, 125, 192)          768       \n",
            " eConv1D)                                                        \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 125, 8)            1544      \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 125, 48)           432       \n",
            "                                                                 \n",
            " depthwise_conv1d_1 (Depthw  (None, 63, 48)            192       \n",
            " iseConv1D)                                                      \n",
            "                                                                 \n",
            " conv1d_4 (Conv1D)           (None, 63, 12)            588       \n",
            "                                                                 \n",
            " conv1d_5 (Conv1D)           (None, 63, 72)            936       \n",
            "                                                                 \n",
            " depthwise_conv1d_2 (Depthw  (None, 32, 72)            288       \n",
            " iseConv1D)                                                      \n",
            "                                                                 \n",
            " conv1d_6 (Conv1D)           (None, 32, 16)            1168      \n",
            "                                                                 \n",
            " conv1d_7 (Conv1D)           (None, 32, 96)            1632      \n",
            "                                                                 \n",
            " depthwise_conv1d_3 (Depthw  (None, 16, 96)            384       \n",
            " iseConv1D)                                                      \n",
            "                                                                 \n",
            " conv1d_8 (Conv1D)           (None, 16, 24)            2328      \n",
            "                                                                 \n",
            " global_average_pooling1d (  (None, 24)                0         \n",
            " GlobalAveragePooling1D)                                         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2)                 50        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16774 (65.52 KB)\n",
            "Trainable params: 16774 (65.52 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define the inverted residual block\n",
        "def inverted_residual_block(x, expansion, stride, filters):\n",
        "    # Expansion Layer\n",
        "    in_channels = x.shape[-1]\n",
        "    x = layers.Conv1D(expansion * in_channels, kernel_size=1, activation='relu')(x)\n",
        "\n",
        "    # Depthwise Convolution\n",
        "    x = layers.DepthwiseConv1D(kernel_size=3, strides=stride, padding='same', activation='relu')(x)\n",
        "\n",
        "    # Projection Layer\n",
        "    x = layers.Conv1D(filters, kernel_size=1, activation=None)(x)\n",
        "\n",
        "    # Add skip connection if the input and output shapes are the same\n",
        "    if stride == 1 and in_channels == filters:\n",
        "        x = layers.add([x, inputs])\n",
        "\n",
        "    return x\n",
        "\n",
        "# Define MobileNetV2 model with specified architecture\n",
        "def MobileNetV2(input_shape, num_classes):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    # Stem Convolution\n",
        "    x = layers.Conv1D(32, kernel_size=3, strides=2, padding='same', activation='relu')(inputs)\n",
        "\n",
        "    # Inverted Residual Blocks\n",
        "    x = inverted_residual_block(x, expansion=6, stride=1, filters=8)\n",
        "    x = inverted_residual_block(x, expansion=6, stride=2, filters=12)\n",
        "    x = inverted_residual_block(x, expansion=6, stride=2, filters=16)\n",
        "    x = inverted_residual_block(x, expansion=6, stride=2, filters=24)\n",
        "\n",
        "    # Global Average Pooling\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "    # Fully Connected Layer\n",
        "    x = layers.Dense(num_classes, activation='sigmoid')(x)\n",
        "\n",
        "    # Create the model\n",
        "    model = models.Model(inputs, x)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Specify input shape and number of classes\n",
        "input_shape = (250, 1)  # Update with your actual input shape\n",
        "num_classes = 2  # Update with the number of classes for your classification task\n",
        "\n",
        "# Create the MobileNetV2 model\n",
        "model = MobileNetV2(input_shape, num_classes)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cc65be1b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc65be1b",
        "outputId": "a3dffd70-149d-406b-ea33-70719f883ae8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "113/114 [============================>.] - ETA: 0s - loss: 0.6211 - accuracy: 0.7168\n",
            "Epoch 1: val_accuracy improved from -inf to 0.68421, saving model to /content/drive/MyDrive/resnet_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r114/114 [==============================] - 6s 22ms/step - loss: 0.6203 - accuracy: 0.7175 - val_loss: 0.6240 - val_accuracy: 0.6842 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "113/114 [============================>.] - ETA: 0s - loss: 0.5952 - accuracy: 0.7195\n",
            "Epoch 2: val_accuracy did not improve from 0.68421\n",
            "114/114 [==============================] - 3s 22ms/step - loss: 0.5953 - accuracy: 0.7193 - val_loss: 0.6237 - val_accuracy: 0.6842 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "114/114 [==============================] - ETA: 0s - loss: 0.5190 - accuracy: 0.7509\n",
            "Epoch 3: val_accuracy improved from 0.68421 to 0.80263, saving model to /content/drive/MyDrive/resnet_model.h5\n",
            "114/114 [==============================] - 2s 19ms/step - loss: 0.5190 - accuracy: 0.7509 - val_loss: 0.4283 - val_accuracy: 0.8026 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "112/114 [============================>.] - ETA: 0s - loss: 0.3463 - accuracy: 0.8446\n",
            "Epoch 4: val_accuracy improved from 0.80263 to 0.85789, saving model to /content/drive/MyDrive/resnet_model.h5\n",
            "114/114 [==============================] - 2s 15ms/step - loss: 0.3448 - accuracy: 0.8456 - val_loss: 0.3418 - val_accuracy: 0.8579 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "113/114 [============================>.] - ETA: 0s - loss: 0.2910 - accuracy: 0.8805\n",
            "Epoch 5: val_accuracy improved from 0.85789 to 0.86842, saving model to /content/drive/MyDrive/resnet_model.h5\n",
            "114/114 [==============================] - 2s 14ms/step - loss: 0.2898 - accuracy: 0.8807 - val_loss: 0.3014 - val_accuracy: 0.8684 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "113/114 [============================>.] - ETA: 0s - loss: 0.2516 - accuracy: 0.8920\n",
            "Epoch 6: val_accuracy improved from 0.86842 to 0.87368, saving model to /content/drive/MyDrive/resnet_model.h5\n",
            "114/114 [==============================] - 2s 14ms/step - loss: 0.2508 - accuracy: 0.8921 - val_loss: 0.2840 - val_accuracy: 0.8737 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "112/114 [============================>.] - ETA: 0s - loss: 0.2042 - accuracy: 0.9268\n",
            "Epoch 7: val_accuracy improved from 0.87368 to 0.90263, saving model to /content/drive/MyDrive/resnet_model.h5\n",
            "114/114 [==============================] - 2s 14ms/step - loss: 0.2021 - accuracy: 0.9281 - val_loss: 0.2107 - val_accuracy: 0.9026 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "112/114 [============================>.] - ETA: 0s - loss: 0.1655 - accuracy: 0.9375\n",
            "Epoch 8: val_accuracy improved from 0.90263 to 0.93947, saving model to /content/drive/MyDrive/resnet_model.h5\n",
            "114/114 [==============================] - 2s 15ms/step - loss: 0.1679 - accuracy: 0.9368 - val_loss: 0.1405 - val_accuracy: 0.9395 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "114/114 [==============================] - ETA: 0s - loss: 0.1235 - accuracy: 0.9579\n",
            "Epoch 9: val_accuracy improved from 0.93947 to 0.97105, saving model to /content/drive/MyDrive/resnet_model.h5\n",
            "114/114 [==============================] - 2s 18ms/step - loss: 0.1235 - accuracy: 0.9579 - val_loss: 0.0739 - val_accuracy: 0.9711 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "113/114 [============================>.] - ETA: 0s - loss: 0.0827 - accuracy: 0.9726\n",
            "Epoch 10: val_accuracy improved from 0.97105 to 0.97895, saving model to /content/drive/MyDrive/resnet_model.h5\n",
            "114/114 [==============================] - 3s 22ms/step - loss: 0.0826 - accuracy: 0.9728 - val_loss: 0.0486 - val_accuracy: 0.9789 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "112/114 [============================>.] - ETA: 0s - loss: 0.0489 - accuracy: 0.9848\n",
            "Epoch 11: val_accuracy improved from 0.97895 to 0.98947, saving model to /content/drive/MyDrive/resnet_model.h5\n",
            "114/114 [==============================] - 2s 18ms/step - loss: 0.0490 - accuracy: 0.9842 - val_loss: 0.0289 - val_accuracy: 0.9895 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 0.0452 - accuracy: 0.9891\n",
            "Epoch 12: val_accuracy did not improve from 0.98947\n",
            "114/114 [==============================] - 1s 13ms/step - loss: 0.0437 - accuracy: 0.9895 - val_loss: 0.0446 - val_accuracy: 0.9816 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "113/114 [============================>.] - ETA: 0s - loss: 0.0270 - accuracy: 0.9920\n",
            "Epoch 13: val_accuracy did not improve from 0.98947\n",
            "114/114 [==============================] - 1s 13ms/step - loss: 0.0280 - accuracy: 0.9912 - val_loss: 0.0473 - val_accuracy: 0.9763 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "111/114 [============================>.] - ETA: 0s - loss: 0.0218 - accuracy: 0.9937\n",
            "Epoch 14: val_accuracy did not improve from 0.98947\n",
            "114/114 [==============================] - 1s 13ms/step - loss: 0.0240 - accuracy: 0.9930 - val_loss: 0.0381 - val_accuracy: 0.9868 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "111/114 [============================>.] - ETA: 0s - loss: 0.0208 - accuracy: 0.9946\n",
            "Epoch 15: val_accuracy improved from 0.98947 to 0.99474, saving model to /content/drive/MyDrive/resnet_model.h5\n",
            "114/114 [==============================] - 2s 14ms/step - loss: 0.0208 - accuracy: 0.9947 - val_loss: 0.0153 - val_accuracy: 0.9947 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "113/114 [============================>.] - ETA: 0s - loss: 0.0154 - accuracy: 0.9965\n",
            "Epoch 16: val_accuracy improved from 0.99474 to 0.99737, saving model to /content/drive/MyDrive/resnet_model.h5\n",
            "114/114 [==============================] - 2s 16ms/step - loss: 0.0153 - accuracy: 0.9965 - val_loss: 0.0106 - val_accuracy: 0.9974 - lr: 0.0010\n",
            "Epoch 17/100\n",
            "114/114 [==============================] - ETA: 0s - loss: 0.0241 - accuracy: 0.9930\n",
            "Epoch 17: val_accuracy did not improve from 0.99737\n",
            "114/114 [==============================] - 2s 14ms/step - loss: 0.0241 - accuracy: 0.9930 - val_loss: 0.0127 - val_accuracy: 0.9947 - lr: 0.0010\n",
            "Epoch 18/100\n",
            "114/114 [==============================] - ETA: 0s - loss: 0.0105 - accuracy: 0.9965\n",
            "Epoch 18: val_accuracy did not improve from 0.99737\n",
            "114/114 [==============================] - 3s 22ms/step - loss: 0.0105 - accuracy: 0.9965 - val_loss: 0.0109 - val_accuracy: 0.9974 - lr: 0.0010\n",
            "Epoch 19/100\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 0.0065 - accuracy: 0.9982\n",
            "Epoch 19: val_accuracy did not improve from 0.99737\n",
            "114/114 [==============================] - 2s 20ms/step - loss: 0.0065 - accuracy: 0.9982 - val_loss: 0.0495 - val_accuracy: 0.9842 - lr: 0.0010\n",
            "Epoch 20/100\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 0.0052 - accuracy: 0.9982\n",
            "Epoch 20: val_accuracy improved from 0.99737 to 1.00000, saving model to /content/drive/MyDrive/resnet_model.h5\n",
            "114/114 [==============================] - 2s 14ms/step - loss: 0.0050 - accuracy: 0.9982 - val_loss: 0.0050 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "Epoch 21/100\n",
            "114/114 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000    \n",
            "Epoch 21: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 14ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "Epoch 22/100\n",
            "114/114 [==============================] - ETA: 0s - loss: 9.2973e-04 - accuracy: 1.0000\n",
            "Epoch 22: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 14ms/step - loss: 9.2973e-04 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "Epoch 23/100\n",
            "114/114 [==============================] - ETA: 0s - loss: 5.4833e-04 - accuracy: 1.0000\n",
            "Epoch 23: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 1s 13ms/step - loss: 5.4833e-04 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "Epoch 24/100\n",
            "112/114 [============================>.] - ETA: 0s - loss: 4.3899e-04 - accuracy: 1.0000\n",
            "Epoch 24: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 1s 13ms/step - loss: 4.3309e-04 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "Epoch 25/100\n",
            "113/114 [============================>.] - ETA: 0s - loss: 3.4777e-04 - accuracy: 1.0000\n",
            "Epoch 25: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 1s 13ms/step - loss: 3.4515e-04 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "Epoch 26/100\n",
            "114/114 [==============================] - ETA: 0s - loss: 2.9358e-04 - accuracy: 1.0000\n",
            "Epoch 26: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 19ms/step - loss: 2.9358e-04 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "Epoch 27/100\n",
            "112/114 [============================>.] - ETA: 0s - loss: 2.3853e-04 - accuracy: 1.0000\n",
            "Epoch 27: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 21ms/step - loss: 2.3504e-04 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "Epoch 28/100\n",
            "112/114 [============================>.] - ETA: 0s - loss: 2.0560e-04 - accuracy: 1.0000\n",
            "Epoch 28: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 15ms/step - loss: 2.0502e-04 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "Epoch 29/100\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.8790e-04 - accuracy: 1.0000\n",
            "Epoch 29: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 1s 13ms/step - loss: 1.8318e-04 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "Epoch 30/100\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.3989e-04 - accuracy: 1.0000\n",
            "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 30: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 14ms/step - loss: 1.5477e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "Epoch 31/100\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.3738e-04 - accuracy: 1.0000\n",
            "Epoch 31: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 1s 13ms/step - loss: 1.3619e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
            "Epoch 32/100\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.3571e-04 - accuracy: 1.0000\n",
            "Epoch 32: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 1s 13ms/step - loss: 1.3375e-04 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
            "Epoch 33/100\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.3306e-04 - accuracy: 1.0000\n",
            "Epoch 33: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 14ms/step - loss: 1.3192e-04 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
            "Epoch 34/100\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.2796e-04 - accuracy: 1.0000\n",
            "Epoch 34: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 14ms/step - loss: 1.2986e-04 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
            "Epoch 35/100\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.2955e-04 - accuracy: 1.0000\n",
            "Epoch 35: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 21ms/step - loss: 1.2808e-04 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
            "Epoch 36/100\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.2506e-04 - accuracy: 1.0000\n",
            "Epoch 36: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 20ms/step - loss: 1.2660e-04 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
            "Epoch 37/100\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.2674e-04 - accuracy: 1.0000\n",
            "Epoch 37: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 1s 13ms/step - loss: 1.2380e-04 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
            "Epoch 38/100\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.2301e-04 - accuracy: 1.0000\n",
            "Epoch 38: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 1s 13ms/step - loss: 1.2194e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
            "Epoch 39/100\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.1851e-04 - accuracy: 1.0000\n",
            "Epoch 39: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 1s 13ms/step - loss: 1.1951e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
            "Epoch 40/100\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.1621e-04 - accuracy: 1.0000\n",
            "Epoch 40: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "\n",
            "Epoch 40: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 1s 13ms/step - loss: 1.1843e-04 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000 - lr: 1.0000e-04\n",
            "Epoch 41/100\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.1521e-04 - accuracy: 1.0000\n",
            "Epoch 41: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 14ms/step - loss: 1.1519e-04 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
            "Epoch 42/100\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.1588e-04 - accuracy: 1.0000\n",
            "Epoch 42: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 1s 12ms/step - loss: 1.1491e-04 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
            "Epoch 43/100\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.1464e-04 - accuracy: 1.0000\n",
            "Epoch 43: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 17ms/step - loss: 1.1464e-04 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
            "Epoch 44/100\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.1634e-04 - accuracy: 1.0000\n",
            "Epoch 44: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 21ms/step - loss: 1.1443e-04 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
            "Epoch 45/100\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.1316e-04 - accuracy: 1.0000\n",
            "Epoch 45: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 3s 23ms/step - loss: 1.1415e-04 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
            "Epoch 46/100\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.1631e-04 - accuracy: 1.0000\n",
            "Epoch 46: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 1s 13ms/step - loss: 1.1388e-04 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
            "Epoch 47/100\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.1453e-04 - accuracy: 1.0000\n",
            "Epoch 47: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 1s 13ms/step - loss: 1.1359e-04 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
            "Epoch 48/100\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.1258e-04 - accuracy: 1.0000\n",
            "Epoch 48: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 1s 13ms/step - loss: 1.1325e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
            "Epoch 49/100\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.1432e-04 - accuracy: 1.0000\n",
            "Epoch 49: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 14ms/step - loss: 1.1309e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
            "Epoch 50/100\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.1448e-04 - accuracy: 1.0000\n",
            "Epoch 50: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "\n",
            "Epoch 50: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 14ms/step - loss: 1.1260e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
            "Epoch 51/100\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.1135e-04 - accuracy: 1.0000\n",
            "Epoch 51: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 15ms/step - loss: 1.1217e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 52/100\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.1213e-04 - accuracy: 1.0000\n",
            "Epoch 52: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 21ms/step - loss: 1.1213e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 53/100\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.0940e-04 - accuracy: 1.0000\n",
            "Epoch 53: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 21ms/step - loss: 1.1210e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 54/100\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.1533e-04 - accuracy: 1.0000\n",
            "Epoch 54: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 15ms/step - loss: 1.1206e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 55/100\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.1086e-04 - accuracy: 1.0000\n",
            "Epoch 55: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 1s 13ms/step - loss: 1.1202e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 56/100\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.1126e-04 - accuracy: 1.0000\n",
            "Epoch 56: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 14ms/step - loss: 1.1198e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 57/100\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.0975e-04 - accuracy: 1.0000\n",
            "Epoch 57: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 1s 13ms/step - loss: 1.1192e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 58/100\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.1495e-04 - accuracy: 1.0000\n",
            "Epoch 58: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 14ms/step - loss: 1.1187e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 59/100\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.1183e-04 - accuracy: 1.0000\n",
            "Epoch 59: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 1s 13ms/step - loss: 1.1183e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 60/100\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.1343e-04 - accuracy: 1.0000\n",
            "Epoch 60: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "\n",
            "Epoch 60: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 19ms/step - loss: 1.1174e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 61/100\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.1022e-04 - accuracy: 1.0000\n",
            "Epoch 61: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 21ms/step - loss: 1.1172e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 62/100\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.1037e-04 - accuracy: 1.0000\n",
            "Epoch 62: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 15ms/step - loss: 1.1165e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 63/100\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.1074e-04 - accuracy: 1.0000\n",
            "Epoch 63: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 14ms/step - loss: 1.1157e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 64/100\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.0857e-04 - accuracy: 1.0000\n",
            "Epoch 64: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 1s 13ms/step - loss: 1.1150e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 65/100\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.1068e-04 - accuracy: 1.0000\n",
            "Epoch 65: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 1s 13ms/step - loss: 1.1145e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 66/100\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.1378e-04 - accuracy: 1.0000\n",
            "Epoch 66: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 1s 13ms/step - loss: 1.1132e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 67/100\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.1125e-04 - accuracy: 1.0000\n",
            "Epoch 67: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 1s 13ms/step - loss: 1.1125e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 68/100\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.1210e-04 - accuracy: 1.0000\n",
            "Epoch 68: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 15ms/step - loss: 1.1116e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 69/100\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.1197e-04 - accuracy: 1.0000\n",
            "Epoch 69: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 21ms/step - loss: 1.1107e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 70/100\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.1191e-04 - accuracy: 1.0000\n",
            "Epoch 70: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 21ms/step - loss: 1.1099e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 71/100\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.1158e-04 - accuracy: 1.0000\n",
            "Epoch 71: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 13ms/step - loss: 1.1088e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 72/100\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.0683e-04 - accuracy: 1.0000\n",
            "Epoch 72: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 15ms/step - loss: 1.1077e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 73/100\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.1159e-04 - accuracy: 1.0000\n",
            "Epoch 73: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 1s 13ms/step - loss: 1.1066e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 74/100\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.1056e-04 - accuracy: 1.0000\n",
            "Epoch 74: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 15ms/step - loss: 1.1056e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 75/100\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.1177e-04 - accuracy: 1.0000\n",
            "Epoch 75: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 13ms/step - loss: 1.1046e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 76/100\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.1142e-04 - accuracy: 1.0000\n",
            "Epoch 76: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 1s 13ms/step - loss: 1.1028e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 77/100\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.1155e-04 - accuracy: 1.0000\n",
            "Epoch 77: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 18ms/step - loss: 1.1015e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 78/100\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.1096e-04 - accuracy: 1.0000\n",
            "Epoch 78: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 21ms/step - loss: 1.1003e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 79/100\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.0664e-04 - accuracy: 1.0000\n",
            "Epoch 79: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 17ms/step - loss: 1.0982e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 80/100\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.1060e-04 - accuracy: 1.0000\n",
            "Epoch 80: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 1s 13ms/step - loss: 1.0967e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 81/100\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.1209e-04 - accuracy: 1.0000\n",
            "Epoch 81: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 1s 13ms/step - loss: 1.0951e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 82/100\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.1075e-04 - accuracy: 1.0000\n",
            "Epoch 82: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 1s 13ms/step - loss: 1.0935e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 83/100\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.0934e-04 - accuracy: 1.0000\n",
            "Epoch 83: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 13ms/step - loss: 1.0918e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 84/100\n",
            "111/114 [============================>.] - ETA: 0s - loss: 1.0942e-04 - accuracy: 1.0000\n",
            "Epoch 84: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 14ms/step - loss: 1.0903e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 85/100\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.0660e-04 - accuracy: 1.0000\n",
            "Epoch 85: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 1s 13ms/step - loss: 1.0882e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 86/100\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.0862e-04 - accuracy: 1.0000\n",
            "Epoch 86: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 21ms/step - loss: 1.0862e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 87/100\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.0843e-04 - accuracy: 1.0000\n",
            "Epoch 87: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 22ms/step - loss: 1.0843e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 88/100\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.0814e-04 - accuracy: 1.0000\n",
            "Epoch 88: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 14ms/step - loss: 1.0814e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 89/100\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 9.7722e-05 - accuracy: 1.0000\n",
            "Epoch 89: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 1s 13ms/step - loss: 1.0791e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 90/100\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.0766e-04 - accuracy: 1.0000\n",
            "Epoch 90: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 13ms/step - loss: 1.0766e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 91/100\n",
            "112/114 [============================>.] - ETA: 0s - loss: 1.0782e-04 - accuracy: 1.0000\n",
            "Epoch 91: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 1s 13ms/step - loss: 1.0755e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 92/100\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.0801e-04 - accuracy: 1.0000\n",
            "Epoch 92: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 13ms/step - loss: 1.0718e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 93/100\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.0679e-04 - accuracy: 1.0000\n",
            "Epoch 93: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 1s 13ms/step - loss: 1.0690e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 94/100\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.0674e-04 - accuracy: 1.0000\n",
            "Epoch 94: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 16ms/step - loss: 1.0674e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 95/100\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.0606e-04 - accuracy: 1.0000\n",
            "Epoch 95: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 21ms/step - loss: 1.0639e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 96/100\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.0694e-04 - accuracy: 1.0000\n",
            "Epoch 96: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 21ms/step - loss: 1.0609e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 97/100\n",
            "110/114 [===========================>..] - ETA: 0s - loss: 1.0498e-04 - accuracy: 1.0000\n",
            "Epoch 97: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 1s 13ms/step - loss: 1.0589e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 98/100\n",
            "114/114 [==============================] - ETA: 0s - loss: 1.0560e-04 - accuracy: 1.0000\n",
            "Epoch 98: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 15ms/step - loss: 1.0560e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 99/100\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.0568e-04 - accuracy: 1.0000\n",
            "Epoch 99: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 1s 13ms/step - loss: 1.0518e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n",
            "Epoch 100/100\n",
            "113/114 [============================>.] - ETA: 0s - loss: 1.0596e-04 - accuracy: 1.0000\n",
            "Epoch 100: val_accuracy did not improve from 1.00000\n",
            "114/114 [==============================] - 2s 14ms/step - loss: 1.0504e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - lr: 1.0000e-06\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Data Preparation\n",
        "# Load and preprocess your ECG dataset here (e.g., PhysioNet Challenge 2017 dataset)\n",
        "# Split the data into training, validation, and test sets\n",
        "\n",
        "# Step 2: Data Preprocessing\n",
        "# Normalize the data\n",
        "# One-hot encode labels if necessary\n",
        "# Create data generators or data loaders for efficient training\n",
        "\n",
        "# Step 3: Model Training Configuration\n",
        "num_epochs = 100  # Adjust as needed\n",
        "batch_size = 10  # Adjust as needed\n",
        "learning_rate = 0.00001  # Adjust as needed\n",
        "checkpoint_path = 'model_checkpoint.h5'  # Path to save model checkpoints\n",
        "\n",
        "\n",
        "\n",
        "optim = tf.optimizers.Adam(learning_rate = learning_rate)\n",
        "\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', mode='max', factor=0.1, patience=10, min_lr=1e-6, verbose=1)\n",
        "\n",
        "checkpoint_filepath = os.path.join(r\"/content/drive/MyDrive\", 'resnet_model.h5')\n",
        "\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True,\n",
        "    verbose=1)\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(inputs_train, outputs_train, validation_data =(inputs_validate, outputs_validate),\n",
        "          epochs = num_epochs, batch_size = batch_size, shuffle=True, callbacks=[reduce_lr, model_checkpoint])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "fca1bbfd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fca1bbfd",
        "outputId": "34014dcf-e6f4-4239-ca2c-1e3eea738b27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 0s 17ms/step - loss: 0.0147 - accuracy: 0.9947\n",
            "Test Loss: 0.014746774919331074\n",
            "Test Accuracy: 0.9947368502616882\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Evaluation\n",
        "test_loss, test_accuracy = model.evaluate(inputs_test, outputs_test)\n",
        "print(f'Test Loss: {test_loss}')\n",
        "print(f'Test Accuracy: {test_accuracy}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "2e200a75",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "2e200a75",
        "outputId": "7a800d02-b9ff-4b98-e86d-ca31f12ebe16"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6.0, 4.0]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# increase the size of the graphs. The default size is (6,4).\n",
        "plt.rcParams[\"figure.figsize\"] = (6,4)\n",
        "\n",
        "# graph the loss, the model above is configure to use \"mean squared error\" as the loss function\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'g.', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(plt.rcParams[\"figure.figsize\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "f033cda1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f033cda1",
        "outputId": "ca489f80-8dcb-43fa-89a5-b54bebd439ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 0s 14ms/step\n",
            "predictions =\n",
            " [[0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.001 0.999]\n",
            " [0.912 0.106]\n",
            " [0.003 0.997]\n",
            " [0.001 0.999]\n",
            " [0.003 0.997]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.006 0.994]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.001 0.999]\n",
            " [0.    1.   ]\n",
            " [0.001 0.999]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.001 0.999]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.001 0.999]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [0.999 0.001]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.001 0.999]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.999 0.001]\n",
            " [0.996 0.004]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [0.999 0.001]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [0.037 0.958]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.001 0.999]\n",
            " [0.001 0.999]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.001 0.999]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.001 0.999]\n",
            " [0.002 0.998]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [1.    0.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]\n",
            " [0.    1.   ]]\n",
            "actual =\n",
            " [[0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "# use the model to predict the test inputs\n",
        "predictions = model.predict(inputs_test)\n",
        "\n",
        "# print the predictions and the expected ouputs\n",
        "print(\"predictions =\\n\", np.round(predictions, decimals=3))\n",
        "print(\"actual =\\n\", outputs_test)\n",
        "y_true = outputs_test\n",
        "y_pred = predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "conf_matrix = confusion_matrix(outputs_test.argmax(axis=1), predictions.argmax(axis=1))\n",
        "# Normalize the confusion matrix to show percentages\n",
        "conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "class_names = [\"Normal\", \"Atrial Fibrillation\"]\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "#sns.heatmap(conf_matrix_normalized, annot=True, fmt=\".2f\", cmap=\"Blues\")\n",
        "sns.heatmap(conf_matrix_normalized, annot=True, fmt=\".1f\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oMZ-agJJtmeX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "dfbeb580-dc1a-42f1-befb-d4159ef7a8a9"
      },
      "id": "oMZ-agJJtmeX",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "classification_rep = classification_report(outputs_test.argmax(axis=1), predictions.argmax(axis=1))\n",
        "print(classification_rep)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eySK8rw0uAr9",
        "outputId": "d8161e0a-55df-4fd6-f5ad-f1660b42f3f4"
      },
      "id": "eySK8rw0uAr9",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99        80\n",
            "           1       1.00      1.00      1.00       300\n",
            "\n",
            "    accuracy                           0.99       380\n",
            "   macro avg       0.99      0.99      0.99       380\n",
            "weighted avg       0.99      0.99      0.99       380\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import recall_score\n",
        "\n",
        "recall = recall_score(outputs_test.argmax(axis=1), predictions.argmax(axis=1), average='weighted')\n",
        "print(f\"Recall: {recall}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oreT7zwPuVyD",
        "outputId": "6ebb75a6-f160-47f2-9b30-e4edc879e071"
      },
      "id": "oreT7zwPuVyD",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall: 0.9947368421052631\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}